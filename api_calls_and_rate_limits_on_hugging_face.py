# -*- coding: utf-8 -*-
"""API Calls and Rate limits on Hugging face.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1thogDESVHzEbgd-t1CX5hPkoLU1IHxdM
"""

# Block 1: Model API Setup with secure token input
import os
from getpass import getpass
from huggingface_hub import InferenceClient
from transformers import AutoTokenizer

# 1️⃣ Prompt user to securely enter Hugging Face token (hidden input)
os.environ["HF_TOKEN"] = getpass("Enter your Hugging Face API token: ")

# 2️⃣ Initialize InferenceClient
MODEL_ID = "mistralai/Mistral-7B-Instruct-v0.2"
client = InferenceClient(
    provider="featherless-ai",
    api_key=os.environ["HF_TOKEN"]
)

# 3️⃣ Load tokenizer for token counting
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)

# 4️⃣ Function to count tokens
def count_tokens(text):
    try:
        return len(tokenizer.encode(text, add_special_tokens=False))
    except:
        return len(text.split())

# ✅ Ready to use `client` for inference safely without exposing your key

# Block 2: Load AG News and create P1, P2, P3
from datasets import load_dataset

# Load AG News train split
dataset = load_dataset("ag_news", split="train").shuffle(seed=42)

# Split into P1, P2, P3
P1 = dataset['text'][:10]       # first 10 prompts
P2 = dataset['text'][10:40]     # next 30 prompts
P3 = dataset['text'][40:140]    # next 100 prompts

# Quick check
print("P1 sample:", P1[:3])
print("P2 sample:", P2[:3])
print("P3 sample:", P3[:3])

# Block 3: System instruction and full prompt preparation

# 1️⃣ Define categories
CATEGORIES = ["Weather", "Sports", "Cinema", "Attacks", "Wars", "Politics", "Economy", "Technology","Other"]

# 2️⃣ One-shot example for guidance
ONE_SHOT = """Example:
News: "The local football team won the championship last night."
Category: Sports
"""

# 3️⃣ System prompt template
SYSTEM_PROMPT = f"""
You are an AI assistant that classifies news headlines or short texts.

### Categories
Choose **exactly one** category from this list:
{', '.join(CATEGORIES)}

If none fit clearly, pick the **closest** category from the list.
If it fits to more than 1 cases, call it "Other"

### Output format
Return only one line:
Category: <one category only>

### Examples
News: "The local football team won the championship last night."
Category: Sports

News: "Heavy rains are expected to continue through the weekend."
Category: Weather

### Instructions
- Do not explain your answer.
- Never invent new categories.
- Output only one line following the format.
"""


# 4️⃣ Function to create full prompt for a single news item
def prepare_full_prompt(news_text):
    """
    Returns the system instruction + one-shot example + news text ready for the model.
    The model will output only one word (the category).
    """
    return f"{SYSTEM_PROMPT}\nNews: \"{news_text}\"\nCategory:"

# Block 4: Call the API and store metrics

import pandas as pd
from tqdm import tqdm
import time

results = []

# Choose prompt set (example shown for P1 — repeat for P2, P3 as needed)
for i, prompt in enumerate(tqdm(P3, desc="Processing P3 prompts")):
    full_prompt = prepare_full_prompt(prompt)

    try:
        start_time = time.time()
        completion = client.chat.completions.create(
            model=MODEL_ID,
            messages=[{"role": "user", "content": full_prompt}])
        latency = time.time() - start_time
        output_text = completion.choices[0].message["content"].strip()
        status_code = 200
    except Exception as e:
        latency = None
        output_text = f"Exception: {str(e)}"
        status_code = None

    # Token counts (optional function defined earlier)
    input_tokens = count_tokens(full_prompt)
    output_tokens = count_tokens(output_text) if status_code == 200 else 0

    results.append({
        "prompt_index": i,
        "prompt_set": "P1",
        "prompt": prompt,
        "full_prompt": full_prompt,
        "response": output_text,
        "input_tokens": input_tokens,
        "output_tokens": output_tokens,
        "latency_s": latency,
        "status_code": status_code
    })

# Convert to DataFrame
df_results = pd.DataFrame(results)
pd.set_option('display.max_colwidth', None)
display(df_results)

from google.colab import sheets
sheet = sheets.InteractiveSheet(df=df_results)